%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}
% graphic package
\usepackage{graphicx}
% image folder link
\graphicspath{ {./images/} }
% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
% inlined bib file
\usepackage{filecontents}


%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Covid 19 Community Mobility Analytic with Graph
}

%for single author (just remove % characters)
\author{
{\rm Hui Ho Yin}\\
The Hong Kong University\\
of Science and Technology\\
hyhuiad@connect.ust.hk
\and
{\rm Tam Ho Sing}\\
The Hong Kong University\\
of Science and Technology\\
hstamac@connect.ust.hk
\and
{\rm Chan Ho Yeung}\\
The Hong Kong University\\
of Science and Technology\\
hychanbe@connect.ust.hk
\and
{\rm Wong Tsz Ho}\\
The Hong Kong University\\
of Science and Technology\\
cenz.wong@connect.ust.hk
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
This work is to find out the important features to predict the US County COVID risk level by leveraging public data from Apple, US State Governments, US County Governments, US Health Department, and US Census. A graph neural network model and some insightful relationships between features will also be presented.
\end{abstract}


%-------------------------------------------------------------------------------
\section{Problem Statement}
%-------------------------------------------------------------------------------
The COVID-19 pandemic, also known as the coronavirus pandemic, was first discovered in 2019. Since the first discovery of COVID, it has been mutating and the variants have become a threat to multiple countries, as of 2 December 2021, there are more than 263 million confirmed cases, 5.22 million confirmed deaths. It has become one of the deadliest virus in history.  ~\cite{coviddata}. \\

By leveraging the Apple mobility data, the US census data, the US state government data, the US county government data, and the US health department data, we aim to predict the next outbreak in the US by predicting outbreak risk level of counties, using previous case number, death number, and mobility number of nearby counties. As a result, a county can have a more accurate expectation on its COVID outbreak risk level in the near future. 

%-------------------------------------------------------------------------------
\section{Dataset Description}
%-------------------------------------------------------------------------------
We have used data from three different sources, namely, Apple, National Bureau of Economic Research (NBER), and the New York Times, as mentioned in the previous sections, the ultimate sources of these sources would be the US census, the US state government, the US county government, and the US health department. ~\cite{applemobility}~\cite{nytimes}~\cite{nber}.\\

For Apple mobility data, there are serval level of data provided, namely, country-level, region-level, subregion-level, county-level. For each layer, there are mobility data of different transportation type, namely, driving, walking, and transit. For each transportation and level of data, mobility trend data starting from 13-Jan-2020 are available.  ~\cite{applemobility}.\\

\begin{tabular}{ |p{1.5cm}||p{4cm}|p{1.3cm}|  }
 \hline
 \multicolumn{3}{|c|}{Datasets} \\
 \hline
 Dataset    & Description & Size\\
 \hline
 Apple Mobility Data & 
    4691 rows of daily mobility data with respect to different geo\_type, transportation\_type since 13-Jan-2020
 & 20MB\\
 \hline
 County Distance Data &   
    Paired county distance with respect to distance constraint of 50 miles, 100 miles, 500 miles, and unlimited 
   & 
    1MB
    4MB
    82MB
    325MB 
 \\
 \hline
 Daily COVID Data  &
 Cumulative daily case and death number of each county since 21-Jan-2020
 & 77MB\\
 \hline
\end{tabular}\\

Apple mobility trend data geo\_type vs transportation type: \\
\begin{tabular}{ |p{2.3cm}||p{1.1cm}|p{1cm}|p{1.2cm}|p{0.7cm}|  }
 \hline
 \multicolumn{5}{|c|}{Apple Mobility Data} \\
 \hline
 Type    & Driving & Transit & Walking & Total\\
 \hline
 City & 299 & 197 & 294 & 790\\
 \hline
 Country/Region & 63 & 27 & 63 & 153\\
 \hline
 County(US) & 2090 & 152 & 396 & 2638\\
 \hline
 Sub-region & 596 & 175 & 339 & 1110\\
 \hline
\end{tabular}\\

We chose county driving data for our project, as it is comparatively the most complete data:  \\
\begin{tabular}{ |p{2.5cm}||p{1.1cm}|p{1.5cm}|p{1cm}|  }
 \hline
 \multicolumn{4}{|c|}{Driving Mobility Data} \\
 \hline
 Type    & Driving & Total & \% \\
 \hline
 City & 299 & 10,000+ & <3\\
 \hline
 Country/Region & 63 & 195 & 32\\
 \hline
 County(US) & 2,090 & 3,006 & 66\\
 \hline
 Sub-region & 596 & unknown & -\\
 \hline
\end{tabular}\\

Number of county pairs with respect of different distance constraint: \\
\begin{tabular}{ |p{3cm}||p{3cm}|  }
 \hline
 \multicolumn{2}{|c|}{County Distance} \\
 \hline
 Distance    & Pairs \\
 \hline
 50 miles & 38,794\\
 \hline
 100 miles & 147,158\\
 \hline
 500 miles & 2,537,351\\
 \hline
 Unlimited & 10,371,621\\
 \hline
\end{tabular}\\

The distance between two counties is not the straight line distance between mid-points of two counties, it is calculated by the greater circle distance between the internal points of two counties.~\cite{nber}~\cite{internalpoint}.\\

Let $\lambda_1$, $\phi_1$, $\lambda_2$, $\phi_2$ be the latitude and longitude of two points, the actual arc length $d$ of this two points is calculated by:

\[d = r\arccos(\sin\phi_1\sin\phi_2+\cos\phi_1\cos\phi_2\cos(|\lambda_2-\lambda_1|))\]

where $r$ is the radius of the Earth.~\cite{countydis}.\\

The internal points of counties are calculated by their latitude and longitude, which is near the geographical centre most of the time. For irregular shaped counties, if the internal points are calculated outside of the boundary of the counties, the actual internal point would be the point in the boundary of the counties which is closest to the calculated internal point.~\cite{internalpoint}.\\

As there are too many pairs if we use unlimited distance, which might affect the efficiency in our project, we decided to not use the unlimited distance pairs. 
%-------------------------------------------------------------------------------
\section{Feature Preprocessing \& Engineering}
%-------------------------------------------------------------------------------
\subsection{Data Cleansing}
\subsubsection{COVID Case / Death Number}
As the COVID case and death number from the New York times are sourced from different organization including US County Governments, US State Governments, and US Health Departments, there are some discrepancy between data of different dates. The case and death number published by the New York Times are cumulative number, however, there are data point where the number is less than the day before that data point. A large discrepancy will affect the result of our model, as a result, we decided to exclude all counties that have discrepancy larger than 30 on any dates.  
\subsubsection{Apple Mobility Data}

As there are counties with the same name, without additional details, it is impossible to distinguish between such counties, we decided to exclude all counties with the same name.

On the other hand, there are counties that were not included in the New York Times COVID case and death number dataset, we decided to also exclude those counties.

\subsection{Joining Datasets}

To create final feature dataset to use in training our GNN model, we inner joined the apple mobility data, the COVID case number, the COVID death number, and the US county code (the FIPS County Code). \\

The county distance dataset will be the node pair dataset, where the weighting of the edges will be the inverse distance of the counties, as the shorter distance between two counties would mean higher importance between the features of those counties.


%-------------------------------------------------------------------------------
\section{Data Visualization}
%-------------------------------------------------------------------------------

A plot of cleansed COVID case number since 21-Jan-2021:
\includegraphics[scale=0.2]{case}\\

A plot of cleansed COVID death number since 21-Jan-2021:
\includegraphics[scale=0.2]{death}\\

We can see that the spike of death is slightly later than the spike of case, which is an expected behavior.

\subsection{Information of Mobility}
Described by Apple, the "Mobility" is generated by counting the number of direction/routing requests the users made on Apple Maps. Apple would like to make this information helpful to the governments and health authorities to generate insights and hopes to make it useful for reference for the new public policies by showing how the volume of people's walking, driving and taking public transit changes in their communities.\\

Apple mainly emphasizes that Apple Maps embraces privacy as core from the beginning. Apple declared that the mobility data collected by their map application, Apple Maps, does not relate to any user's Apple ID, and Apple does not store the location history of any user. So, in the mobility data set we downloaded, we can just see mobility per countries/regions, and cannot see any personal identifier columns. \\

\subsection{Filter Out Mobility's Weekly Seasonality}
As shown as below, there are many regular periodic changes along the mobility, which makes us difficult to truly see the trend of the mobility data. In this case, the seasonality is by weekly and it is related to people movements which repeat every week. For example, people will go to workplace from Monday to Friday, go home after works but go out for dinning on Friday, and go out for hanging or playing during weekend.\\

To remove the seasonality, in this case, it is a weekly seasonality, using 7-day moving average can filter out this seasonality. The original and filtered results are shown below. It is much clear to see the trend of the mobility across time.

Before:

\includegraphics[scale=0.2]{images/seasonality-before.png}\\
After:

\includegraphics[scale=0.2]{images/seasonality-after.png}\\


\subsection{Mobility and Daily Confirmed Cases}
The below plot shows the major waves of confirmed cases in U.S. There are 6 obvious peaks of confirmed cases in COVID-19, where 2 major peaks appear in the period of October 2020 to mid February 2021 and July to November 2021. There is a great drop in mobility of all three types of transportation type at the end of March 2020, because of the public declaration of COVID-19 as pandemic by WHO. \\

Line chart - Mobility and Daily Confirmed Cases in U.S.:

\includegraphics[scale=0.15]{images/mobility-cases-line-chart}

\subsection{Timeline of COVID-19 in U.S.}
The below plots show the mobility plus the number of confirmed cases in U.S. from 13rd January to 19th May 2020. Several major events are interested to look into. \\

Significant events related to COVID-19 in U.S.:

\includegraphics[scale=0.2]{images/timeline-1.png}\\
\includegraphics[scale=0.16]{images/timeline-2.png}\\

On January 21, 2020, Centers for Disease Control and Prevention (CDC) reported a first confirmed case of COVID-19 in the state of Washington. It is travel-related and the patient was returned from Wuhan, China on January 15, 2020. The mobility keeps nearly the same level which indicates U.S. residents did not have a great reaction on it. \\

On February 24, President at that time, Donald Trump, tweeted that "The Coronavirus is very much under control in the USA", when he was visiting India. The mobility of Apple users greatly soared after the day he made this tweet in Twitter from February 24 to March 11, 2020. But the mobility of taking public transit decreased, which indicates people might still concern about the epidemic and would like to avoid public transportation. \\

However, on March 11, World Health Organization (WHO) that WHO "made the assessment that COVID-19 can be characterized as a pandemic." User mobility decreased significantly after WHO's declaration. On March 13, Trump also announced a national emergency to deal with coronavirus crisis. The mobility keeps dropping until the end of March while the confirmed cases increases.\\

On March 30, Arizona, one of the states in U.S., issued stay-at-home order and it took effect on March 31. The mobility did not decrease so much after that. On May 11, Arizona state allowed restaurants to open dining rooms, the mobility of driving skyrockets on March 11.

\subsection{Insight - Change in Behaviors of Taking Transportation}
In the normal period (Jan 2020), the portion of taking public transportation is about 8.5\%. After WHO declared COVID-19 as pandemic, the portion of taking public transports decreases more than 4\% to around 4\%. Until nowadays, from November 2021 to now, although portion of public transit increases to 5.5\%, it is still lower than before while portion of driving and walking increase. It shows that people avoid to take public transportation, and they are more willing to walk or drive compared to before-epidemic. It makes sense, because the coronavirus is much easier to spread from the infected to others in a crowded and closed area.\\

Portion of three transportation types during different periods:

\includegraphics[scale=0.12]{images/mobility-pie-all.png}\\


%-------------------------------------------------------------------------------
\section{Graph Visualization}
%-------------------------------------------------------------------------------
Since all the three graphs contain a huge amount of nodes and edges, it is difficult to draw the graph explicitly. Instead, the distribution of degree, path length and clustering coefficient are analyzed for three graphs with different distance constraints, which are 50 miles, 100 miles and 500 miles. These three graphs are denoted as g50m, g100m and g500m respectively. 

\subsection{Degree Distribution}
The degree of a node is defined as the number of neighbours it connects to.\\

First of all, the average degree and the degree distribution are investigated. The average degrees of g50m, g100m and g500m are 12.49, 45.91 and 819.05 respectively. The value of average degrees increase with the distance constraint, since higher distance constraint implies that each node (county) is connected with more neighbours. \\

The degree distributions in linear scale and log scale of g50m:\\
\includegraphics[scale=0.6]{Deg50}
\includegraphics[scale=0.6]{Deglog50}

From the plot in linear scale, it is observed that the degree distribution of g50m first follows the binomial distribution at a low number of degree (0 to 20), then follows power-law starting from a degree of 20. The log-log plot also shows that it follows power-law at a high degree. \\

The degree distributions in linear scale and log scale of g100m:\\
\includegraphics[scale=0.6]{deg100}
\includegraphics[scale=0.6]{deglog100}

The linear and log scale plot illustrate that the degree distribution of g100m follows neither binomial distribution nor power-law, but rather some abnormal distribution, with the mean shifted towards the high degree side. \\

The degree distributions in linear scale and log scale of g500m:\\
\includegraphics[scale=0.6]{deg500}
\includegraphics[scale=0.6]{deglog500}

It seems that the degree distribution of g500m follows power-law by just looking at the linear plot, but the log-log plot illustrates that the distribution is also unusual as of that of g100m. \\

\subsection{Path Length}
The path length between two nodes is defined as the smallest number of edges needed to travelled from one node to another. \\

The average path length and its distribution are analyzed for each of the graphs. First, the average path length of g50m, g100m and g500m are 17.35, 9.92 and 2.21 respectively. The path length of g50m does not follow small-world phenomenon since the distance constraint is too small, implying that a node is connected to a small number of nodes and have to travel a larger distance to reach the desired destination. The other two graphs both satisfy the small-world phenomenon with a path length approximate to 6~7. \\ 

The path length distribution of g50m:\\
\includegraphics[scale=0.6]{path50}
The path length of g50m follows a binomial distribution but with the mean shifted towards the left, and the maximum path length is around 50. Such a high path length exists because some nodes are far away from other nodes in terms of the distance between counties, and at the same time the edges only connect those with small inter-county distance due to the distance constraint, and so more edges are needed to travel from one node to another. \\

The path length distribution of g100m:\\
\includegraphics[scale=0.6]{path100}
The maximum path length of g100m is about 30, which is less than that of g50m since the distance constrain is relaxed. The distribution is similar to binomial distribution but also with the mean shifted towards the left. \\

The path length distribution of g500m:\\
\includegraphics[scale=0.6]{path500}
This time the maximum path length is greatly reduced to 6, and most of the path length concentrates on the region of 2~3 since the distance constraint is released to 500m. \\

\subsection{Clustering Coefficient}
The clustering coefficient, which measures how close the neighbours of a node is, is also evaluated. The formula for clustering coefficient of node i is 
\[C_i = \frac{2e_i}{k_i(k_i-1)}\]
where $e_i$ is the number of edges between the neighbours of node i, and $k_i$ is the number of neighbours of node i. \\

First, the average clustering coefficient of g50, g100m and g500m are 0.48, 0.59 and 0.72. The larger the distance constraint, the higher the clustering coefficient is for more connections between nodes. \\

The clustering coefficient distribution of g50m:\\
\includegraphics[scale=0.6]{cluster50}
For g50m, the highest clustering coefficient is 1, and ,and most of the nodes have a clustering coefficient between 0.4 to 0.6, which is reasonable for a real-world graph.\\

The clustering coefficient distribution of g100m:\\
\includegraphics[scale=0.6]{cluster100}
The clustering coefficient distribution of g100m is similar to that of g50m, with the mean slightly moved to the right between 0.5 to 0.7, and the highest clustering coefficient is also 1. \\

The clustering coefficient distribution of g500m:\\
\includegraphics[scale=0.6]{cluster500}
This time the lowest clustering coefficient is 0.5, and the highest is 1. It implies that the nodes are more closely packed compared to g50m and g100m, since the distance constrain is less strict. \\

In conclusion for the graph analysis, the following table summarise the three major characteristics of the three graphs g50m, g100m and g500m.\\

\begin{tabular}{ |p{1cm}||p{2cm}|p{2cm}|p{2cm}|  }
 \hline
 \multicolumn{4}{|c|}{Characteristics of the graphs} \\
 \hline
 Graph  & Avg. Degree & Avg. Path length & Avg. Clustering coefficient \\
 \hline
 g50m & 12.49 & 17.35 & 0.48\\
 \hline
 g100m & 45.91 & 9.92 & 0.59\\
 \hline
 g500m & 819.05 & 2.21 & 0.72\\
 \hline
\end{tabular}\\

%-------------------------------------------------------------------------------
 \section{Graph Machine Learning}
%-------------------------------------------------------------------------------
Graph Machine Learning is different from traditional Machine Learning in the following aspect.\\

First, graph data is in term of the vertex and the edge where the traditional dataset only has tabular relation, resulting that the dimension of the graph data is higher than the one with tabular data.\\

Next, there are more target for graph machine learning, for example, in graph data, we can perform link prediction, node prediction, subgraph clustering and so much more while in tradition machine learning, it would be much simpler for predicting the target label.\\

In the traditional way, we have CNN model to handle data with higher dimension while in graph network, it is hard to do that, since the data in graph would be in sparse metric where a lot of zeros in the input value resulting impossible for a classic CNN to compute.\\

A GNN is introduced to solve this problem. Graph network is a more arbitrary space when it compares to the Matrix form of data. First, the graph is encoded to a lower dimension using Graph Representation Learning. After transforming a graph into lower dimension array, it then pass into a CNN network to train the model to do the prediction.\\

Hence, graph machine learning can preserve the relation information between different node, resulting it a better model to describe the problem.\\

Graph Machine Learning is becoming more and more valuable since a Graph data can hold more information than the traditional tabular data format. In real world problem, most of our problem we are facing can be formulated using a network. \\

\includegraphics[scale=0.25]{images/CNNGNN.png}
 ~\cite{CNNGNN}\\

In this project, we are using the COVID19 Apple Mobility Data which by nature is a network data. The location of different places and the distance between them is indeed forming a graph. In this report we will like to investigate the relationship between the mobility data and the confirming case daily. By joining tables from the mobility data, location mapping and the confirming cases together, we can have the following graph data.\\

\begin{tabular}{ |p{2cm}|p{2cm}| }
 \hline
 \multicolumn{2}{|c|}{Graph Data as Input of the Machine Learning} \\
 \hline
 \textbf{Name}    & \textbf{Meaning}  \\
 \hline
 Edge & Distance  \\
 \hline
 Node & County  \\
 \hline
 Node Feature & Mobility Data \\
 \hline
  Output & Confirm Cases \\
 \hline
\end{tabular}\\

According to the Novel Coronavirus Situation Report from WHO ~\cite{whocovid19}, the incubation period of the COVID19 would be mainly from 2 to 14 days with possible outliers up to 27 days. For the node feature, we pick the past 14 days Mobility data to predict the next day confirmation case. \\

After defining the problem and input/output of the model, we look for different Python package to achieve our goal. AutoGL ~\cite{guan2021autogl} is a Python package for researchers to quickly autoML on the graph dataset. This is a good module to get started with, however, we found that there are not much variable or setting for us to fine tune the model. Therefore, we choose another package for achieving our task.\\

GraphConv ~\cite{kipf2017semisupervised}, Graph Convolutional Networks, is being introduced in 2016 by Thomas N. Kipf, Max Welling. GraphConv is good at handling semi-supervised classification and it scales linearly with the size of the graph and encode both the local graph structure and the features of nodes. We think that this would be the best model in our cases. The formula of GCN is as following:

\[h_i^{(l+1)} = \sigma(b^{(l)} + \sum_{j\in\mathcal{N}(i)}\frac{e_{ji}}{c_{ji}}h_j^{(l)}W^{(l)})\]

The formula concludes what is happening for the graph convolution. The \(\sigma\) is the activation function of the convolution. \(b\) is the bias of the function and it is also a learnable parameter to test. \(e_{ji}\) is the weight of the edge nodes, in our case, it is the distance between two county in US. \(c_{ji}\) is the square root of the node degree product together and it's formula is as the following

\[c_{ji} = \sqrt{|\mathcal{N}(j)|}\sqrt{|\mathcal{N}(i)|}\]

In graph convolution, we can see that the model itself take consideration on the graph's node degree where traditional convolution do not takes into account on different node. While the traditional convolution takes into account for the spatial dimension information, the graph convolution takes into account for the node property including the node degree and the relation of between node. That's add additional information for the data to the model.

\subsection{Preprocessing}
We implement as a datalake approach to share and processing the data. Dataset would be in flat file format and it is sitting on a cloud storage for better utilize the data between different parties. In order to transform the data to what is takes from the dgl, we have to perform some transformation of the data. We first load the data in the csv format and transform load it into the Python Pandas package, then we use \texttt{map()} function to transform the data into favorable format in parallel manner.\\

After preparing the data, we then load the data into graph format. Module \texttt{dgl} ~\cite{wang2019dgl} use \texttt{networkX} ~\cite{SciPyProceedings_11} as the graph processing module. We then create an empty \texttt{networkX} graph object to store the graph. 

\begin{verbatim}
import networkx as nx
import dgl

nx_G = nx.Graph()
for i in ls_node:
    nx_G.add_node(i)
    
for i in range(len(df_src)):
    nx_G.add_weighted_edges_from(
        [(df_src[i], df_dest[i], 1.0)]
    )

dgl_G = dgl.from_networkx(nx_G)
\end{verbatim}

From the above data processing, we filter out the county which is not appear in the data and only kept the one contain data from the COVID19 mobility data. After all the filtering the network property is shown as the following:

\begin{tabular}{ |p{2cm}|p{2cm}| }
 \hline
 \multicolumn{2}{|c|}{Graph Data Property} \\
 \hline
 \textbf{Property}    & \textbf{number}  \\
 \hline
 Node & 662  \\
 \hline
 Edge & 2398  \\
 \hline
\end{tabular}\\

As mention earlier in the report, we have chosen the way that there will be a link connecting between county if their distance between two place are within 50 miles. Within these 662 network nodes, we have to embedded the node feature for the network. We have to arrange the daily mobility data as the feature of the the model and the daily cases for the labels of the model in order to let the model to perform the graph classification task.

\subsection{Model Training}

We have the following few parameter needs to be decide. First, we need to determine the number of layer that the model have. Typically, we have test that one layer of the GraphConv model is too simple for such large dataset and hence we have add  more layer to GraphConv for higher order network ~\cite{morris2021weisfeiler} , making our model to do the prediction as a two-layer model. An dropout layer is introduced to prevent the model to be overfitting. Like the traditional Convolution Network, here we still have the pooling layer where we aggregate the layer to extract the feature hidden within our data. We pick the max pooling here in order to extract the key feature the model get.\\


% \begin{verbatim}
% class GINConv_L1L2(nn.Module):
%     def __init__(self, 
%             g, in_dim, hidden1_dim, 
%             hidden2_dim, out_dim
%         ):
%         super(GINConv_L1L2, self).__init__()
        
%         self.g = g
%         lin = nn.Linear(in_dim, hidden1_dim)
%         self.layer1 = GINConv(lin, 'max')
        
%         lin = nn.Linear(hidden1_dim, hidden2_dim)
%         self.layer2 = GINConv(lin, 'max')

%         lin = nn.Linear(hidden2_dim, out_dim)
%         self.layer3 = GINConv(lin, 'max')
    
%     def forward(self, h):
%         h = self.layer1(self.g, h)
%         h = F.relu(h)

%         h = self.layer2(self.g, h)
%         self.dropout = torch.nn.Dropout(p=0.1)
%         h = F.relu(h)
        
%         h = self.layer3(self.g, h)
%         return h
% \end{verbatim}

We then have to pick the size of the hidden dimension. Our criteria would be the dimension is as close as the number of the graph size which is near 662 node. Hence we choose the dimension of the hidden dimension as 512. And both hidden layer are of 512 dimension. Another hyperparameter is needed is our learning rate, we have choose a very small learning rate for the model where we think that for a graph data, a large learning rate training will result in missing the suboptimal solution. Below is the summary of the model we use in this project.\\

\begin{verbatim}
GINConv_L1L2(
  (layer1): GINConv(
    (apply_func): Linear(
            in_features=14, 
            out_features=512, 
            bias=True)
    )
  (layer2): GINConv(
    (apply_func): Linear(
            in_features=512, 
            out_features=512, 
            bias=True)
    )
  (layer3): GINConv(
    (apply_func): Linear(
            in_features=512, 
            out_features=2, 
            bias=True)
    )
  (dropout): Dropout(
            p=1e-05, 
            inplace=False
    )
)
\end{verbatim}

Note that the GINConv ~\cite{xu2019powerful} is the modification version of the GraphConv and the formula is as the following:

\[h_i^{(l+1)} = f_\Theta \left((1 + \epsilon) h_i^{l} +
\mathrm{aggregate}\left(\left\{e_{ji} h_j^{l}, j\in\mathcal{N}(i)
\right\}\right)\right)\]

After trial and error on all of the hyper-parameters, we have tested out the best model and the model looks acceptable. We will discuss it in the next section.\\

\subsection{Result}

We have choose the training epoch as 5000 for the model to be trained. Loss of each epoch is being logged to keep track of the overall training process. And we have the following result: 
\includegraphics[scale=0.32]{images/newplot_L1L2.png}

At the first few epoch of training, the model got the overall loss around 76.4 and after 100 epoch, the loss has drop to a half to around 30. One more 100 epoch step forward, the loss has reach to 1.03 which is really acceptable. We then let the model keep running for a optimal result.\\

\subsection{Future Improvement}
Although we might find a good model to predict what we need, we found that the dataset itself has some problem to be fixed. First, there are some imbalance dataset between the case and the model is simply having mostly a high risk alert to that area. Since overall time period in US, they are always in a very high risk situation, resulting that the model is good at performing the high risk cases while the low risk cases is fail to be predicted. \\

The data pipeline of our design is not really optimized for the performance, the time needed for us to load and run the data is relatively long. If we are having more data point, it takes unacceptable long time to train and run the model. The way that we can alleviate the problem is the proper design the entire infrastructure.\\

Currently, we are using the free Colab resources provided by Google. The computing power of the free plan is limited. It would be nice if we can leverage the cloud compute resource on the Cloud Infrastructure vendor like AWS or Azure. Next, we are not running the model on the GPU. Instead, we are running the graph machine learning model on CPU. Not having the model run on GPU makes the training time very long. GPU can allow some of the processing to be run in parallel and hence speeding up the processing time.\\

\subsubsection{Graph Database}

Last changes would be we can use the graph database like neo4j. Neo4j can handle some of the request from us, saving us time on changing different graph representation format. About the databases, we can also use Apache TinkerPop, which is a graph computing framework for both graph databases (OLTP) and graph analytic systems (OLAP) ~\cite{Gremlin}. Using such database dedicated design for the graph dataset can save us time for doing the data cleaning job.\\

\subsubsection{Using Apache Spark}

Apart from using python as our base language, we can also adopt Apache Spark GraphX library ~\cite{10.1145/2934664}. By introducing the distributed computing, we can handle larger graph dataset with same time required. Apache Spark is a in-memory distributing computing framework which is by nature can handle big data data while the graph datasets always comes with huge network. GraphX would be another popular choice for us to try as well.

\subsubsection{Selecting Temporal Model}
The dataset is actually a time-series dataset while we, at the beginning, has ignore the temporal component and transform it into a non time-series feature and labels. It might have some information loss when we ignore the temporal component. In the future, we might further evaluate using different model which can be preserved the temporal component of the dataset.\\

T-GCN is an option for us to try in the future. T-GCN ~\cite{8809901} means a Temporal Graph Convolutional Network. The author of the model suggest that in order to preserve the temporal part of the dataset, we have to add some temporal model to the GCN, for example, a GRU ~\cite{cho2014learning} network or a LSTM ~\cite{LSTM} network. The implementation of the T-GCN is not in DGL library.\\

\subsubsection{Other Graph Module}

We found that other than the DGL module, there are other python module aiming at writing graph machine learning algorithm. StellarGraph ~\cite{StellarGraph} is one of them. It is a graph machine learning library which is similar to DGL. While in the real world problem always comes with spatio-temporal in nature, using a graph machine learning with the capability of extract temporal information is very useful for dealing with such problem. \\

We believe that model with combined graph convolutional network (GCN) and the gated recurrent unit (GRU) would provide us better result on predicting the future trend of the data.

%-------------------------------------------------------------------------------
 \section{Conclusion}
%-------------------------------------------------------------------------------
People change their behaviors of transportation during the epidemic, they prefer to walk or drive by themselves rather than taking public transportation, which makes sense for them to reduce person-to-person physical contact to others in public closed area.\\

With the rise of machine learning, more specifically graph machine learning, researchers find a new way to better define problems with complex network and make predictions of them. Our target of this project is to first visualize the COVID situation in different US counties using different graph metrics, then we will try to build a GNN model that predicts the risk level of outbreak in each county. \\

For the modelling part, we will be focusing on the following relationships: 
\begin{itemize}
    \item Predicting the risk level of having a COVID outbreak in the near future base on feature input
    \item Find the relationship between different features 
\end{itemize} 
We expect to conclude the relationship between features, node distance, and outbreak risk level, and thus helping counties to better prepare on public health in the near future. 

%-------------------------------------------------------------------------------
\section{Team Contributions}
%-------------------------------------------------------------------------------
\begin{tabular}{ |p{2.5cm}||p{4cm}|  }
 \hline
 \multicolumn{2}{|c|}{Contributions} \\
 \hline
 Name    & Description\\
 \hline
 HUI Ho Yin & Researching,Data Engineering, Data Cleansing\\
 \hline
 TAM Ho Sing & Data Visualization / some Data Cleansing, Report Drafting \\
 \hline
 CHAN Ho Yeung &  Data Visualization, Report Drafting\\
 \hline
 WONG Tsz Ho & Data Engineering, GNN \\
 \hline
\end{tabular}\\

%-------------------------------------------------------------------------------
\bibliographystyle{ieeetr}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks
