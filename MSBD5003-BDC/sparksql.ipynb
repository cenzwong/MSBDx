{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(name='Alice', age=11)\n",
      "Alice 11\n",
      "Alice 11\n",
      "<built-in method count of Row object at 0x7ff184030258>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print(row)\n",
    "print(row['name'], row['age'])\n",
    "print(row.name, row.age)\n",
    "\n",
    "row = Row(name=\"Alice\", age=11, count=1)\n",
    "print(row.count)\n",
    "print(row['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file at https://www.cse.ust.hk/msbd5003/data/building.csv\n",
    "\n",
    "df = spark.read.csv('../data/building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BuildingMgr: string (nullable = true)\n",
      " |-- BuildingAge: integer (nullable = true)\n",
      " |-- HVACproduct: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'),\n",
       " Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'),\n",
       " Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "dfrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|BuildingID|     Country|\n",
      "+----------+------------+\n",
      "|         1|         USA|\n",
      "|         2|      France|\n",
      "|         3|      Brazil|\n",
      "|         4|     Finland|\n",
      "|         5|   Hong Kong|\n",
      "|         6|   Singapore|\n",
      "|         7|South Africa|\n",
      "|         8|   Australia|\n",
      "|         9|      Mexico|\n",
      "|        10|       China|\n",
      "|        11|     Belgium|\n",
      "|        12|     Finland|\n",
      "|        13|Saudi Arabia|\n",
      "|        14|     Germany|\n",
      "|        15|      Israel|\n",
      "|        16|      Turkey|\n",
      "|        17|       Egypt|\n",
      "|        18|   Indonesia|\n",
      "|        19|      Canada|\n",
      "|        20|   Argentina|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|BuildingID| OK|\n",
      "+----------+---+\n",
      "|         2| OK|\n",
      "|         3| OK|\n",
      "|         4| OK|\n",
      "|         5| OK|\n",
      "|         6| OK|\n",
      "|         7| OK|\n",
      "|         8| OK|\n",
      "|         9| OK|\n",
      "|        10| OK|\n",
      "|        11| OK|\n",
      "|        12| OK|\n",
      "|        13| OK|\n",
      "|        14| OK|\n",
      "|        15| OK|\n",
      "|        16| OK|\n",
      "|        17| OK|\n",
      "|        18| OK|\n",
      "|        19| OK|\n",
      "|        20| OK|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.where(\"Country<'USA'\").select('BuildingID', lit('OK')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACProduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    4|\n",
      "|     AC1000|    4|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "# Data files at https://www.cse.ust.hk/msbd5003/data\n",
    "\n",
    "dfCustomer = spark.read.csv('../data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('../data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('../data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('../data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+---------+\n",
      "|ProductID|Name                         |ListPrice|\n",
      "+---------+-----------------------------+---------+\n",
      "|680      |HL Road Frame - Black, 58    |1431.5   |\n",
      "|708      |Sport-100 Helmet, Black      |34.99    |\n",
      "|722      |LL Road Frame - Black, 58    |337.22   |\n",
      "|723      |LL Road Frame - Black, 60    |337.22   |\n",
      "|724      |LL Road Frame - Black, 62    |337.22   |\n",
      "|736      |LL Road Frame - Black, 44    |337.22   |\n",
      "|737      |LL Road Frame - Black, 48    |337.22   |\n",
      "|738      |LL Road Frame - Black, 52    |337.22   |\n",
      "|743      |HL Mountain Frame - Black, 42|1349.6   |\n",
      "|744      |HL Mountain Frame - Black, 44|1349.6   |\n",
      "|745      |HL Mountain Frame - Black, 48|1349.6   |\n",
      "|746      |HL Mountain Frame - Black, 46|1349.6   |\n",
      "|747      |HL Mountain Frame - Black, 38|1349.6   |\n",
      "|765      |Road-650 Black, 58           |782.99   |\n",
      "|766      |Road-650 Black, 60           |782.99   |\n",
      "|767      |Road-650 Black, 62           |782.99   |\n",
      "|768      |Road-650 Black, 44           |782.99   |\n",
      "|769      |Road-650 Black, 48           |782.99   |\n",
      "|770      |Road-650 Black, 52           |782.99   |\n",
      "|775      |Mountain-100 Black, 38       |3374.99  |\n",
      "+---------+-----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+------------+\n",
      "|ProductID|Name                         |Double price|\n",
      "+---------+-----------------------------+------------+\n",
      "|680      |HL Road Frame - Black, 58    |2863.0      |\n",
      "|708      |Sport-100 Helmet, Black      |69.98       |\n",
      "|722      |LL Road Frame - Black, 58    |674.44      |\n",
      "|723      |LL Road Frame - Black, 60    |674.44      |\n",
      "|724      |LL Road Frame - Black, 62    |674.44      |\n",
      "|736      |LL Road Frame - Black, 44    |674.44      |\n",
      "|737      |LL Road Frame - Black, 48    |674.44      |\n",
      "|738      |LL Road Frame - Black, 52    |674.44      |\n",
      "|743      |HL Mountain Frame - Black, 42|2699.2      |\n",
      "|744      |HL Mountain Frame - Black, 44|2699.2      |\n",
      "|745      |HL Mountain Frame - Black, 48|2699.2      |\n",
      "|746      |HL Mountain Frame - Black, 46|2699.2      |\n",
      "|747      |HL Mountain Frame - Black, 38|2699.2      |\n",
      "|765      |Road-650 Black, 58           |1565.98     |\n",
      "|766      |Road-650 Black, 60           |1565.98     |\n",
      "|767      |Road-650 Black, 62           |1565.98     |\n",
      "|768      |Road-650 Black, 44           |1565.98     |\n",
      "|769      |Road-650 Black, 48           |1565.98     |\n",
      "|770      |Road-650 Black, 52           |1565.98     |\n",
      "|775      |Mountain-100 Black, 38       |6749.98     |\n",
      "+---------+-----------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct['ListPrice'] * 2).alias('Double price')) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------+\n",
      "|ProductID|Name                     |(ListPrice * 2)|\n",
      "+---------+-------------------------+---------------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0         |\n",
      "|706      |HL Road Frame - Red, 58  |2863.0         |\n",
      "|717      |HL Road Frame - Red, 62  |2863.0         |\n",
      "|718      |HL Road Frame - Red, 44  |2863.0         |\n",
      "|719      |HL Road Frame - Red, 48  |2863.0         |\n",
      "|720      |HL Road Frame - Red, 52  |2863.0         |\n",
      "|721      |HL Road Frame - Red, 56  |2863.0         |\n",
      "|722      |LL Road Frame - Black, 58|674.44         |\n",
      "|723      |LL Road Frame - Black, 60|674.44         |\n",
      "|724      |LL Road Frame - Black, 62|674.44         |\n",
      "|725      |LL Road Frame - Red, 44  |674.44         |\n",
      "|726      |LL Road Frame - Red, 48  |674.44         |\n",
      "|727      |LL Road Frame - Red, 52  |674.44         |\n",
      "|728      |LL Road Frame - Red, 58  |674.44         |\n",
      "|729      |LL Road Frame - Red, 60  |674.44         |\n",
      "|730      |LL Road Frame - Red, 62  |674.44         |\n",
      "|731      |ML Road Frame - Red, 44  |1189.66        |\n",
      "|732      |ML Road Frame - Red, 48  |1189.66        |\n",
      "|733      |ML Road Frame - Red, 52  |1189.66        |\n",
      "|734      |ML Road Frame - Red, 58  |1189.66        |\n",
      "+---------+-------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.ListPrice * 2 > 100) \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+---------+\n",
      "|ProductID|Name                      |ListPrice|\n",
      "+---------+--------------------------+---------+\n",
      "|860      |Half-Finger Gloves, L     |24.49    |\n",
      "|859      |Half-Finger Gloves, M     |24.49    |\n",
      "|858      |Half-Finger Gloves, S     |24.49    |\n",
      "|708      |Sport-100 Helmet, Black   |34.99    |\n",
      "|862      |Full-Finger Gloves, M     |37.99    |\n",
      "|861      |Full-Finger Gloves, S     |37.99    |\n",
      "|863      |Full-Finger Gloves, L     |37.99    |\n",
      "|841      |Men's Sports Shorts, S    |59.99    |\n",
      "|849      |Men's Sports Shorts, M    |59.99    |\n",
      "|851      |Men's Sports Shorts, XL   |59.99    |\n",
      "|850      |Men's Sports Shorts, L    |59.99    |\n",
      "|815      |LL Mountain Front Wheel   |60.745   |\n",
      "|868      |Women's Mountain Shorts, M|69.99    |\n",
      "|869      |Women's Mountain Shorts, L|69.99    |\n",
      "|867      |Women's Mountain Shorts, S|69.99    |\n",
      "|853      |Women's Tights, M         |74.99    |\n",
      "|854      |Women's Tights, L         |74.99    |\n",
      "|852      |Women's Tights, S         |74.99    |\n",
      "|818      |LL Road Front Wheel       |85.565   |\n",
      "|823      |LL Mountain Rear Wheel    |87.745   |\n",
      "+---------+--------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   20.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\") \\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113278|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71936|            113228|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71902|            112980|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71797|            111075|Sport-100 Helmet,...|   20.994|       6|\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71782|            110709|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71938|            113295|Sport-100 Helmet,...|   20.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71938|            113282|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71902|            112995|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71863|            112395|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71797|            111038|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71784|            110753|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71782|            110708|Sport-100 Helmet,...|   20.994|       6|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   20.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [SalesOrderID#217, SalesOrderDetailID#218, Name#168, UnitPrice#221, OrderQty#219]\n",
      "+- *(2) BroadcastHashJoin [ProductID#220], [ProductID#167], Inner, BuildLeft\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#380]\n",
      "   :  +- *(1) Project [SalesOrderID#217, SalesOrderDetailID#218, OrderQty#219, ProductID#220, UnitPrice#221]\n",
      "   :     +- *(1) Filter isnotnull(ProductID#220)\n",
      "   :        +- FileScan csv [SalesOrderID#217,SalesOrderDetailID#218,OrderQty#219,ProductID#220,UnitPrice#221] Batched: false, DataFilters: [isnotnull(ProductID#220)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- *(2) Project [ProductID#167, Name#168]\n",
      "      +- *(2) Filter ((isnotnull(Color#170) AND (Color#170 = Black)) AND isnotnull(ProductID#167))\n",
      "         +- FileScan csv [ProductID#167,Name#168,Color#170] Batched: false, DataFilters: [isnotnull(Color#170), (Color#170 = Black), isnotnull(ProductID#167)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.show()\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0;\n'Filter ('Color = Black)\n+- Relation[SalesOrderID#580,SalesOrderDetailID#581,Name#582,UnitPrice#583,OrderQty#584] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-168595bb0376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Color = 'Black'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \"\"\"\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0;\n'Filter ('Color = Black)\n+- Relation[SalesOrderID#580,SalesOrderDetailID#581,Name#582,UnitPrice#583,OrderQty#584] csv\n"
     ]
    }
   ],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('temp.csv', header = True, inferSchema = True)\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "|       71831|\n",
      "|       71898|\n",
      "|       71935|\n",
      "|       71938|\n",
      "|       71845|\n",
      "|       71783|\n",
      "|       71815|\n",
      "|       71936|\n",
      "|       71863|\n",
      "|       71780|\n",
      "|       71782|\n",
      "|       71899|\n",
      "|       71784|\n",
      "|       71797|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71915|1732.8899999999999|\n",
      "|       71946|            31.584|\n",
      "|       71895|221.25600000000003|\n",
      "|       71816|2847.4079999999994|\n",
      "|       71831|          1712.946|\n",
      "|       71923|         96.108824|\n",
      "|       71858|11528.844000000001|\n",
      "|       71917|            37.758|\n",
      "|       71897|          10585.05|\n",
      "|       71885|           524.664|\n",
      "|       71856|500.30400000000003|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71774|           713.796|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71935|5533.8689079999995|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "|       71897|          10585.05|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "|       71783|      65683.367986|\n",
      "|       71936| 79589.61602399996|\n",
      "|       71780|29923.007999999998|\n",
      "|       71782| 33319.98600000001|\n",
      "|       71784| 89869.27631400003|\n",
      "|       71797| 65123.46341800001|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|26677.883999999995|\n",
      "|       71832|      16883.748108|\n",
      "|       71938|         33779.448|\n",
      "|       71845|         18109.836|\n",
      "|       71783|15524.117476000003|\n",
      "|       71936|      44490.290424|\n",
      "|       71780|         16964.322|\n",
      "|       71797|      27581.613792|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct.where(\"Color = 'Black'\"), 'ProductID') \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|CustomerID|   FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+------------+------------+-------------+\n",
      "|     30050|     Krishna|Sunkammurali|           89|\n",
      "|     29796|         Jon|      Grande|           65|\n",
      "|     29957|       Kevin|         Liu|           62|\n",
      "|     29929|     Jeffrey|       Kurtz|           46|\n",
      "|     29546| Christopher|        Beck|           45|\n",
      "|     29922|      Pamala|        Kotc|           34|\n",
      "|     30113|        Raja|   Venugopal|           34|\n",
      "|     29938|       Frank|    Campbell|           29|\n",
      "|     29736|       Terry|   Eminhizer|           23|\n",
      "|     29485|   Catherine|        Abel|           10|\n",
      "|     30019|     Matthew|      Miller|            9|\n",
      "|     29932|     Rebecca|      Laszlo|            7|\n",
      "|     29975|      Walter|        Mays|            5|\n",
      "|     29638|    Rosmarie|     Carroll|            2|\n",
      "|     30089|Michael John|      Troyer|            1|\n",
      "|     29531|        Cory|       Booth|            1|\n",
      "|     29568|      Donald|     Blanton|            1|\n",
      "|     29861|     Phyllis|    Huntsman|         null|\n",
      "|     29719|    Patricia|       Doyle|         null|\n",
      "|     29866|     Richard|       Irwin|         null|\n",
      "+----------+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataframe as a temporary view called HVAC\n",
    "df.createOrReplaceTempView('HVAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HVACproduct|count(1)|\n",
      "+-----------+--------+\n",
      "|    ACMAX22|       3|\n",
      "|     AC1000|       3|\n",
      "|     JDNS77|       4|\n",
      "|     FN39TG|       4|\n",
      "|     GG1919|       4|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\n",
    "spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACproduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    3|\n",
      "|     AC1000|    3|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\n",
    "d1.groupBy('HVACproduct').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   5|\n",
      "|         2|         M2|         27|     FN39TG|      France|   8|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   8|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   9|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|  11|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|  11|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  14|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|  11|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   8|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   7|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   9|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   9|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  14|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   9|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   8|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   8|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   7|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|  11|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   8|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|  11|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slen = udf(lambda s: len(s)+2, IntegerType())\n",
    "df.select('*', slen(df['Country']).alias('slen')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   3|\n",
      "|         2|         M2|         27|     FN39TG|      France|   6|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   6|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   7|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|   9|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|   9|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  12|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|   9|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   6|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   5|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   7|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   7|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  12|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   7|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   6|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   6|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   5|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|   9|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   6|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|   9|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('slen', lambda s: len(s), IntegerType())\n",
    "spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|      dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|[9.5, 7.0, 12.0]|  2|An ice sculpture| 12.5|[cold, ice]|   [-78.75, 20.4]|\n",
      "| [1.0, 3.1, 1.0]|  3|    A blue mouse| 25.5|       null|    [54.4, -32.7]|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)),\n",
       " Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Converting between RDD and DataFrame\n",
    "\n",
    "Sample data file at:\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Michael', 29), ('Andy', 30), ('Justin', 19)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"../data/people.txt\")\n",
    "\n",
    "def parse(l):\n",
    "    a = l.split(',')\n",
    "    return (a[0], int(a[1]))\n",
    "\n",
    "rdd = lines.map(parse)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples, schema is inferred\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of Rows, type is given in the Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_rows = rdd.map(lambda p: Row(name = p[0], age = p[1]))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| name| age|\n",
      "+-----+----+\n",
      "|Alice|  11|\n",
      "|  Bob|null|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row fields with types incompatible with that of previous rows will be turned into nulls\n",
    "row1 = Row(name=\"Alice\", age=11)\n",
    "row2 = Row(name=\"Bob\", age='12')\n",
    "rdd_rows = sc.parallelize([row1, row2])\n",
    "df1 = spark.createDataFrame(rdd_rows)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: Justin']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "teenagers = df.filter('age >= 13 and age <= 19')\n",
    "\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "teenNames.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "DataFrames are stored using columnar storage with compression\n",
    "\n",
    "RDDs are stored using row storage without compression\n",
    "\n",
    "The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(_1#1265L) AND (_1#1265L < 5))\n",
      "+- *(1) Scan ExistingRDD[_1#1265L,_2#1266L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(((cast((_1#1265L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#1296]\n",
      "+- *(1) Scan ExistingRDD[_1#1265L,_2#1266L]\n",
      "\n",
      "\n",
      "+----------------------------+\n",
      "|((((_1 * 2) + 2.5) + 1) + 1)|\n",
      "+----------------------------+\n",
      "|                         4.5|\n",
      "|                         6.5|\n",
      "|                         8.5|\n",
      "|                        10.5|\n",
      "|                        12.5|\n",
      "|                        14.5|\n",
      "|                        16.5|\n",
      "|                        18.5|\n",
      "|                        20.5|\n",
      "|                        22.5|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * 2 + f() + 1 + 1)\n",
    "df1.explain()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print(a.take(10))\n",
    "x = 3\n",
    "print(a.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
